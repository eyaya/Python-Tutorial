{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The \"pandas\" module is specifically useful for computing and finding patterns in data and hence it is one of the most useful tools for any data scientist/engineer. However, most of the time you would not be using pandas alone, but you would possibly be using modules like __scipy__ and __matplotlib__ along with pandas to achieve your ends. Here, we will be focusing on pandas only, but in our examples we will make use of scipy and __matplotlib__ to display data distribution and such things.\n",
    "\n",
    "Pandas provides the data scientist/analyst with three different (and very useful data structures) named \"series\", \"dataframe\" and \"panel\". \n",
    "\n",
    "\n",
    "## Series\n",
    "\n",
    "A “series” is nothing but a labelled column of data that can hold any datatype (int, float, string, objects, etc). A pandas “series” can be created using the following constructor call:\n",
    "\n",
    "```python\n",
    "pandas.Series(data, index, dtype, copy)\n",
    "```\n",
    "The argument \"data\" is a list of data elements (mostly passed as a numpy ndarray), \"index\" is a unique hashable list with the same length as the \"data\" argument. \"dtype\" defines the data type (Series is a homogeneous collection of elements), and \"copy\" specifies if copy flag is set. By default, this is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "Pandas is a popular Python package for data science, and with good reason: it offers powerful, expressive and flexible data structures that make data manipulation and analysis easy, among many other things. The DataFrame is one of these structures.\n",
    "\n",
    "DataFrames in Python are very similar: they come with the Pandas library, and they are defined as two-dimensional labeled data structures with columns of potentially different types.\n",
    "\n",
    "In general, you could say that the Pandas DataFrame consists of three main components: the data, the index, and the columns.\n",
    "\n",
    "\n",
    "1. The DataFrame can contain data that is:\n",
    "<ul>\n",
    "    <li>a Pandas DataFrame</li>\n",
    "    <li>a Pandas Series: a one-dimensional labeled array capable of holding any data type with axis labels or index. An example of a Series object is one column from a DataFrame.</li>\n",
    "    <li>a NumPy ndarray, which can be a record or structured<li>\n",
    "    <li>a two-dimensional ndarray</li>\n",
    "    <li>dictionaries of one-dimensional ndarray's, lists, dictionaries or Series.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides data, you can also specify the index and column names for your DataFrame. The index, on the one hand, indicates the difference in rows, while the column names indicate the difference in columns. You will see later that these two components of the DataFrame will come in handy when you’re manipulating your data.\n",
    "\n",
    " ### Creating, Reading and Writing\n",
    " \n",
    " ### Getting started\n",
    "\n",
    "To use pandas, you'll typically start with the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data\n",
    "\n",
    "There are two core objects in pandas: the DataFrame and the Series.\n",
    "\n",
    "\n",
    "### Series\n",
    "\n",
    "A Series, by contrast, is a sequence of data values. If a DataFrame is a table, a Series is a list. And in fact you can create one with nothing more than a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, 2, 3, 4, 5], index=['A','B', 'C','D','E'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series is, in essence, a single column of a DataFrame. So you can assign column values to the Series the same way as before, using an index parameter. However, a Series does not have a column name, it only has one overall name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign names to our values__\n",
    "\n",
    "Pandas will automatically generate our indexes, so we need to define them. Each index corresponds to its value in the Series object. Let’s look at an example where we assign a country name to population growth rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.Series([30, 35.5, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.index.name = \"Year\" # Set index name \n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.name=\"New Products\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Select entries from a Series__\n",
    "\n",
    "To select entries from a Series, we select elements based on the index name or index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[\"2016 Sales\"] #Fetch element at index named 2016 Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[[0,2]]   #Fetch elements at multiple indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop entries from a Series__\n",
    "\n",
    "Dropping and unwanted index is a common function in Pandas. If the drop(index_name) function is called with a given index on a Series object, the desired index name is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Original Data\", Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.drop(\"2016 Sales\") # Drop index named 2016 Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Data \",Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DataFrame__\n",
    "\n",
    "A DataFrame is a table. It contains an array of individual entries, each of which has a certain value. Each entry corresponds to a row (or record) and a column.\n",
    "\n",
    "For example, consider the following simple DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]}, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the \"0, No\" entry has the value of 131. The \"0, Yes\" entry has a value of 50, and so on.\n",
    "\n",
    "DataFrame entries are not limited to integers. For instance, here's a DataFrame whose values are strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Abebe': ['ወድጀዋለሁ.', 'It was awful.'], 'Kebede': ['Pretty good.', 'Bland.']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pd.DataFrame() constructor to generate these DataFrame objects. The syntax for declaring a new one is a dictionary whose keys are the column names (Bob and Sue in this example), and whose values are a list of entries. This is the standard way of constructing a new DataFrame, and the one you are most likely to encounter.\n",
    "\n",
    "The dictionary-list constructor assigns values to the column labels, but just uses an ascending count from 0 $(0, 1, 2, 3,\\cdots)$ for the row labels. Sometimes this is OK, but oftentimes we will want to assign these labels ourselves.\n",
    "\n",
    "The list of row labels used in a DataFrame is known as an Index. We can assign values to it by using an index parameter in our constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'አበበ': ['ወድጀዋለሁ.', 'It was awful.'], \n",
    "              'ከበደ': ['Pretty good.', 'Bland.']},\n",
    "             index=['Product A', 'Product B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading data files__\n",
    "\n",
    "Being able to create a DataFrame or Series by hand is handy. But, most of the time, we won't actually be creating our own data by hand. Instead, we'll be working with data that already exists.\n",
    "\n",
    "Data can be stored in any of a number of different forms and formats. By far the most basic of these is the humble CSV file.\n",
    "\n",
    "Let's now set aside our toy datasets and see what a real dataset looks like when we read it into a DataFrame. We'll use the __pd.read_csv()__ function to read the data into a DataFrame. This goes thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews = pd.read_csv(\"wd.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the shape attribute to check how large the resulting DataFrame is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our new DataFrame has 150,000 records split across 14 different columns. That's almost 2 million entries!\n",
    "\n",
    "We can examine the contents of the resultant DataFrame using the __head()__ command, which grabs the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pd.read_csv() function is well-endowed, with over 30 optional parameters you can specify. For example, you can see in this dataset that the CSV file has a built-in index, which pandas did not pick up on automatically. To make pandas use that column for the index (instead of creating a new one from scratch), we can specify an index_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an index or Column from a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can access the property of an object by accessing it as an attribute. A book object, for example, might have a title property, which we can access by calling book.title. Columns in a pandas DataFrame work in much the same way.\n",
    "\n",
    "Hence to access the country property of reviews we can use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points # wine_reviews[\"points\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a Python dictionary, we can access its values using the indexing ([]) operator. We can do the same with columns in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews[['country','points']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[-10:,[0,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the two ways of selecting a specific Series out of a DataFrame. Neither of them is more or less syntactically valid than the other, but the indexing operator [] does have the advantage that it can handle column names with reserved characters in them (e.g. if we had a country providence column, reviews.country providence wouldn't work).\n",
    "\n",
    "Doesn't a pandas Series look kind of like a fancy dictionary? It pretty much is, so it's no surprise that, to drill down to a single specific value, we need only use the indexing operator [] once more:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['country'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing in Pandas\n",
    "The indexing operator and attribute selection are nice because they work just like they do in the rest of the Python ecosystem. As a novice, this makes them easy to pick up and use. However, pandas has its own accessor operators, loc and iloc. For more advanced operations, these are the ones you're supposed to be using.\n",
    "#### Index-based selection\n",
    "\n",
    "Pandas indexing works in one of two paradigms. The first is index-based selection: selecting data based on its numerical position in the data. iloc follows this paradigm.\n",
    "\n",
    "To select the first row of data in a DataFrame, we may use the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[0:5,[3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both _loc_ and _iloc_ are row-first, column-second. This is the opposite of what we do in native Python, which is column-first, row-second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that it's marginally easier to retrieve rows, and marginally harder to get retrieve columns. To get a column with iloc, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On its own, the : operator, which also comes from native Python, means \"everything\". When combined with other selectors, however, it can be used to indicate a range of values. For example, to select the country column from just the first, second, and third row, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[:3, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, to select just the second and third entries, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[1:3, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to pass a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[np.arange(0,10,2), 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's worth knowing that negative numbers can be used in selection. This will start counting forwards from the end of the values. So for example here are the last five elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['province']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.iloc[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Label-based selection__\n",
    "\n",
    "The second paradigm for attribute selection is the one followed by the loc operator: label-based selection. In this paradigm, it's the data index value, not its position, which matters.\n",
    "\n",
    "For example, to get the first entry in reviews, we would now do the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[4, 'country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__iloc__ is conceptually simpler than __loc__ because it ignores the dataset's indices. When we use iloc we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. loc, by contrast, uses the information in the indices to do its work. Since your dataset usually has meaningful indices, it's usually easier to do things using loc instead. For example, here's one operation that's much easier using loc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[:5, ['points','variety','winery']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating the index\n",
    "\n",
    "Label-based selection derives its power from the labels in the index. Critically, the index we use is not immutable. We can manipulate the index in any way we see fit.\n",
    "\n",
    "The set_index() method can be used to do the job. Here is what happens when we set_index to the title field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.set_index(\"province\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional selection\n",
    "\n",
    "So far we've been indexing various strides of data, using structural properties of the DataFrame itself. To do interesting things with the data, however, we often need to ask questions based on conditions.\n",
    "\n",
    "For example, suppose that we're interested specifically in better-than-average wines produced in Italy.\n",
    "\n",
    "We can start by checking if each wine is Italian or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews[wine_reviews.price>87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation produced a Series of True/False booleans based on the country of each record. This result can then be used inside of loc to select the relevant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews[wine_reviews.country == 'Italy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has ~20,000 rows. The original had ~150,000. That means that around 13% of wines originate from Italy.\n",
    "\n",
    "We also wanted to know which ones are better than average. Wines are reviewed on a 80-to-100 point scale, so this could mean wines that accrued at least 90 points.\n",
    "\n",
    "We can use the ampersand (&) to bring the two questions together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[(wine_reviews.country == 'Italy') & (wine_reviews.points >= 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'll buy any wine that's made in Italy or which is rated above average. For this we use a pipe (|):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[(wine_reviews.country == 'Italy') | (wine_reviews.points >= 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas comes with a few built-in conditional selectors, two of which we will highlight here.\n",
    "\n",
    "The first is isin. isin is lets you select data whose value \"is in\" a list of values. For example, here's how we can use it to select wines only from Italy or France:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_excel('https://api.worldbank.org/v2/en/country/ETH?downloadformat=excel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.columns = Data.iloc[2,:]\n",
    "\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.iloc[3:,:]\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.reset_index(inplace=True)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Data.columns[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(A.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.columns[5:] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[wine_reviews.country.isin(['Italy', 'France'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is __isnull__ (and its companion __notnull__). These methods let you highlight values which are (or are not) empty (NaN). For example, to filter out wines lacking a price tag in the dataset, here's what we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.loc[wine_reviews.region_2.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can get to the solution, it’s first a good idea to grasp the concept of loc and how it differs from other indexing attributes such as __.iloc[]__ and __.ix[]__:\n",
    "\n",
    "__loc[]__ works on labels of your index. This means that if you give in __loc[2]__, you look for the values of your DataFrame that have an index labeled 2.\n",
    "\n",
    "__iloc[]__ works on the positions in your index. This means that if you give in __iloc[2]__, you look for the values of your DataFrame that are at index ’2`.\n",
    "\n",
    "__ix[]__ is a more complex case: when the index is integer-based, you pass a label to __.ix[]__. __ix[2]__ then means that you’re looking in your DataFrame for values that have an index labeled 2. This is just like __.loc[]__! However, if your index is not solely integer-based, __ix__ will work with positions, just like __.iloc[]__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning data\n",
    "\n",
    "Going the other way, assigning data to a DataFrame is easy. You can assign either a constant value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['critic'] = 'everyone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with an iterable of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews['critic'] = range(len(wine_reviews), 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary functions\n",
    "\n",
    "Pandas provides many simple \"summary functions\" (not an official name) which restructure the data in some useful way. For example, consider the describe() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method generates a high-level summary of the attributes of the given column. It is type-aware, meaning that its output changes based on the data type of the input. The output above only makes sense for numerical data; for string data here's what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.winery.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get some particular simple summary statistic about a column in a DataFrame or a Series, there is usually a helpful pandas function that makes it happen.\n",
    "\n",
    "For example, to see the mean of the points allotted (e.g. how well an averagely rated wine does), we can use the mean() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points.quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of unique values we can use the unique() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wine_reviews.winery.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of unique values and how often they occur in the dataset, we can use the value_counts() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.winery.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Maps\n",
    "\n",
    "A map is a term, borrowed from mathematics, for a function that takes one set of values and \"maps\" them to another set of values. In data science we often have a need for creating new representations from existing data, or for transforming data from the format it is in now to the format that we want it to be in later. Maps are what handle this work, making them extremely important for getting your work done!\n",
    "\n",
    "There are two mapping methods that you will use often.\n",
    "\n",
    "map() is the first, and slightly simpler one. For example, suppose that we wanted to remean the scores the wines received to 0. We can do this as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_points_mean = wine_reviews.points.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_points_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points - review_points_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.points.map(lambda p: p - review_points_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function you pass to __map()__ should expect a single value from the Series (a point value, in the above example), and return a transformed version of that value. map() returns a new Series where all the values have been transformed by your function.\n",
    "\n",
    "__apply()__ is the equivalent method if we want to transform a whole DataFrame by calling a custom method on each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.apply(remean_points, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had called reviews.apply() with __axis='index'__, then instead of passing a function to transform each row, we would need to give a function to transform each column.\n",
    "\n",
    "Note that map() and apply() return new, transformed Series and DataFrames, respectively. They don't modify the original data they're called on. If we look at the first row of reviews, we can see that it still has its original points value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides many common mapping operations as built-ins. For example, here's a faster way of remeaning our points column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_points_mean = wine_reviews.points.mean()\n",
    "wine_reviews.points - review_points_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.country + \" - \" + wine_reviews.region_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupwise analysis\n",
    "\n",
    "One function we've been using heavily thus far is the value_counts() function. We can replicate what value_counts() does by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_reviews.groupby('points').points.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging with Pandas\n",
    "\n",
    "Merging is used when we want to collect data that shares a key variable but are located in different DataFrames. To merge DataFrames, we use the merge() function. Say we have df1 and df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'subject_id': ['1', '2', '3', '4', '5'],\n",
    "    'student_name': ['Mark', 'Khalid', 'Deborah', 'Trevon', 'Raven']\n",
    "}\n",
    "df1 = pd.DataFrame(d, columns=['subject_id', 'student_name'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'subject_id': ['4', '5', '6', '7', '8'],\n",
    "    'student_name': ['Eric', 'Imani', 'Cece', 'Darius', 'Andre']\n",
    "}\n",
    "df2 = pd.DataFrame(data, columns=['subject_id', 'student_name'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we merge them? It’s simple: with the merge() function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on='student_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping with Pandas\n",
    "\n",
    "Grouping is how we categorize our data. If a value occurs in multiple rows of a single column, the data related to that value in other columns can be grouped together. Just like with merging, it’s more simple than it sounds. We use the groupby function. Look at this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = {\n",
    "    'Name': ['Darell', 'Darell', 'Lilith', 'Lilith', 'Tran', 'Tran', 'Tran',\n",
    "        'Tran', 'John', 'Darell', 'Darell', 'Darell'],\n",
    "    'Position': [2, 1, 1, 4, 2, 4, 3, 1, 3, 2, 4, 3],\n",
    "    'Year': [2009, 2010, 2009, 2010, 2010, 2010, 2011, 2012, 2011, 2013, 2013, 2012],\n",
    "    'Marks':[408, 398, 422, 376, 401, 380, 396, 388, 356, 402, 368, 378]\n",
    "}\n",
    "df = pd.DataFrame(raw)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df.groupby('Year')\n",
    "group.get_group(2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation\n",
    "\n",
    "Concatenation is a long word that means to add a set of data to another. We use the concat() function to do so. To clarify the difference between merge and concatenation, merge() combines data on shared columns, while concat() combines DataFrames across columns or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data sets from a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = (\"https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv\")\n",
    "df = pd.read_csv(download_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Your First Pandas Plot\n",
    "\n",
    "Your dataset contains some columns related to the earnings of graduates in each major:\n",
    "<ul>\n",
    "    <li> \"Median\" is the median earnings of full-time, year-round workers.</li>\n",
    "    <li> \"P25th\" is the 25th percentile of earnings.</li>\n",
    "    <li> \"P75th\" is the 75th percentile of earnings.</li>\n",
    "    <li> \"Rank\" is the majors rank by median earnings.</li>\n",
    "</ul>\n",
    "Let's start with a plot displaying these columns. First, you need to set up your Jupyter Notebook to display plots with the __%matplotlib__ magic command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Rank\", y=[\"P25th\", \"Median\", \"P75th\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, you can make the following observations:\n",
    "<ul>\n",
    "    <li>The median income decreases as rank decreases. This is expected because the rank is determined by the median income.</li>\n",
    "    <li>Some majors have large gaps between the 25th and 75th percentiles. People with these degrees may earn significantly less or significantly more than the median income.</li>\n",
    "    <li>Other majors have very small gaps between the 25th and 75th percentiles. People with these degrees earn salaries very close to the median income.</li>\n",
    "</ul>\n",
    "Your first plot already hints that there's a lot more to discover in the data! Some majors have a wide range of earnings, and others have a rather narrow range. To discover these differences, you'll use several other types of plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".plot() has several optional parameters. Most notably, the kind parameter accepts eleven different string values and determines which kind of plot you’ll create:\n",
    "<ol>\n",
    "    \n",
    "    \"area\" is for area plots.\n",
    "    \"bar\" is for vertical bar charts.\n",
    "    \"barh\" is for horizontal bar charts.\n",
    "    \"box\" is for box plots.\n",
    "    \"hexbin\" is for hexbin plots.\n",
    "    \"hist\" is for histograms.\n",
    "    \"kde\" is for kernel density estimate charts.\n",
    "    \"density\" is an alias for \"kde\".\n",
    "    \"line\" is for line graphs.\n",
    "    \"pie\" is for pie charts.\n",
    "    \"scatter\" is for scatter plots.\n",
    "</ol>\n",
    "The default value is __\"line\"__. Line graphs, like the one you created above, provide a good overview of your data. You can use them to detect general trends. They rarely provide sophisticated insight, but they can give you clues as to where to zoom in.\n",
    "\n",
    "If you don’t provide a parameter to .plot(), then it creates a line plot with the index on the x-axis and all the numeric columns on the y-axis. While this is a useful default for datasets with only a few columns, for the college majors dataset and its several numeric columns, it looks like quite a mess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions and Histograms\n",
    "\n",
    "DataFrame is not the only class in pandas with a .plot() method. As so often happens in pandas, the Series object provides similar functionality.\n",
    "\n",
    "You can get each column of a DataFrame as a Series object. Here’s an example using the \"Median\" column of the DataFrame you created from the college major data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_column = df[\"Median\"]\n",
    "type(median_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_column.plot(kind=\"hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows the data grouped into ten bins ranging from $\\$20,000$ to $\\$120,000$, and each bin has a width of $10,000. The histogram has a different shape than the normal distribution, which has a symmetric bell shape with a peak in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of the median data, however, peaks on the left below $\\$40,000$. The tail stretches far to the right and suggests that there are indeed fields whose majors can expect significantly higher earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Have you spotted that lonely small bin on the right edge of the distribution? It seems that one data point has its own category. The majors in this field get an excellent salary compared not only to the average but also to the runner-up. Although this isn't its main purpose, a histogram can help you to detect such an outlier. Let's investigate the outlier a bit more:\n",
    "<ul>\n",
    "    <li>Which majors does this outlier represent?</li>\n",
    "    <li>How big is its edge?</li>\n",
    "</ul>\n",
    "Contrary to the first overview, you only want to compare a few data points, but you want to see more details about them. For this, a bar plot is an excellent tool. First, select the five majors with the highest median earnings. You'll need two steps:\n",
    "<ul>\n",
    "    <li>To sort by the \"Median\" column, use .sort_values() and provide the name of the column you want to sort by as well as the direction ascending=False.</li>\n",
    "    <li>To get the top five items of your list, use .head().</li>\n",
    "</ul>\n",
    "Let's create a new DataFrame called top_5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = df.sort_values(by=\"Median\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a smaller DataFrame containing only the top five most lucrative majors. As a next step, you can create a bar plot that shows only the majors with these top five median salaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5.plot(x=\"Major\", y=\"Median\", kind=\"bar\", rot=45, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you use the rot and fontsize parameters to rotate and size the labels of the x-axis so that they're visible. You'll see a plot with 5 bars:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that the median salary of petroleum engineering majors is more than $20,000 higher than the rest. The earnings for the second- through fourth-place majors are relatively close to one another.\n",
    "\n",
    "If you have a data point with a much higher or lower value than the rest, then you’ll probably want to investigate a bit further. For example, you can look at the columns that contain related data.\n",
    "\n",
    "Let’s investigate all majors whose median salary is above $60,000. First, you need to filter these majors with the mask _df[df[\"Median\"] > 60000]_. Then you can create another bar plot showing all three earnings columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_medians = df[df[\"Median\"] > 60000].sort_values(\"Median\")\n",
    "top_medians.plot(x=\"Major\", y=[\"P25th\", \"Median\", \"P75th\"], kind=\"bar\",)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 25th and 75th percentile confirm what you’ve seen above: petroleum engineering majors were by far the best paid recent graduates.\n",
    "\n",
    "Why should you be so interested in outliers in this dataset? If you’re a college student pondering which major to pick, you have at least one pretty obvious reason. But outliers are also very interesting from an analysis point of view. They can indicate not only industries with an abundance of money but also invalid data.\n",
    "\n",
    "Invalid data can be caused by any number of errors or oversights, including a sensor outage, an error during the manual data entry, or a five-year-old participating in a focus group meant for kids age ten and above. Investigating outliers is an important step in data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the data is correct, you may decide that it’s just so different from the rest that it produces more noise than benefit. Let’s assume you analyze the sales data of a small publisher. You group the revenues by region and compare them to the same month of the previous year. Then out of the blue, the publisher lands a national bestseller.\n",
    "\n",
    "This pleasant event makes your report kind of pointless. With the bestseller’s data included, sales are going up everywhere. Performing the same analysis without the outlier would provide more valuable information, allowing you to see that in New York your sales numbers have improved significantly, but in Miami they got worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Correlation\n",
    "\n",
    "Often you want to see whether two columns of a dataset are connected. If you pick a major with higher median earnings, do you also have a lower chance of unemployment? As a first step, create a scatter plot with those two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"Median\", y=\"Unemployment_rate\", kind=\"scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a quite random-looking plot, like this: scatter plot median unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance at this figure shows that there’s no significant correlation between the earnings and unemployment rate.\n",
    "\n",
    "While a scatter plot is an excellent tool for getting a first impression about possible correlation, it certainly isn’t definitive proof of a connection. For an overview of the correlations between different columns, you can use .corr(). If you suspect a correlation between two values, then you have several tools at your disposal to verify your hunch and measure how strong the correlation is.\n",
    "\n",
    "Keep in mind, though, that even if a correlation exists between two values, it still doesn’t mean that a change in one would result in a change in the other. In other words, correlation does not imply causation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Categorical Data\n",
    "\n",
    "To process bigger chunks of information, the human mind consciously and unconsciously sorts data into categories. This technique is often useful, but it’s far from flawless.\n",
    "\n",
    "Sometimes we put things into a category that, upon further examination, aren’t all that similar. In this section, you’ll get to know some tools for examining categories and verifying whether a given categorization makes sense.\n",
    "\n",
    "Many datasets already contain some explicit or implicit categorization. In the current example, the 173 majors are divided into 16 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "A basic usage of categories is grouping and aggregation. You can use .groupby() to determine how popular each of the categories in the college major dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_totals = df.groupby(\"Major_category\")[\"Total\"].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_totals.plot(kind=\"barh\", fontsize=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your plot shows, business is by far the most popular major category. While humanities and liberal arts is the clear second, the rest of the fields are more similar in popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Ratios\n",
    "\n",
    "Vertical and horizontal bar charts are often a good choice if you want to see the difference between your categories. If you’re interested in ratios, then pie plots are an excellent tool. However, since cat_totals contains a few smaller categories, creating a pie plot with cat_totals.plot(kind=\"pie\") will produce several tiny slices with overlapping labels .\n",
    "\n",
    "To address this problem, you can lump the smaller categories into a single group. Merge all categories with a total under 100,000 into a category called \"Other\", then create a pie plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cat_totals = cat_totals[cat_totals < 100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_cat_totals = cat_totals[cat_totals > 100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new item \"Other\" with the sum of the small categories\n",
    "small_sums = pd.Series([small_cat_totals.sum()], index=[\"Other\"])\n",
    "big_cat_totals = big_cat_totals.append(small_sums)\n",
    "big_cat_totals.plot(kind=\"pie\", label=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you include the argument label=\"\". By default, pandas adds a label with the column name. That often makes sense, but in this case it would only add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "data = np.random.normal(0, 1, 3)\n",
    "# array([-1.18878589,  0.59627021,  1.59895721])plt.figure(figsize=(16, 6))\n",
    "sns.boxplot(x=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "\n",
    "# prepare some data\n",
    "x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "y0 = [i**2 for i in x]\n",
    "y1 = [10**i for i in x]\n",
    "y2 = [10**(i**2) for i in x]\n",
    "\n",
    "# output to static HTML file\n",
    "output_file(\"log_lines.html\")\n",
    "\n",
    "# create a new plot\n",
    "p = figure(\n",
    "   tools=\"pan,box_zoom,reset,save\",\n",
    "   y_axis_type=\"log\", y_range=[0.001, 10**11], title=\"log axis example\",\n",
    "   x_axis_label='sections', y_axis_label='particles'\n",
    ")\n",
    "\n",
    "# add some renderers\n",
    "p.line(x, x, legend=\"y=x\")\n",
    "p.circle(x, x, legend=\"y=x\", fill_color=\"white\", size=8)\n",
    "p.line(x, y0, legend=\"y=x^2\", line_width=3)\n",
    "p.line(x, y1, legend=\"y=10^x\", line_color=\"red\")\n",
    "p.circle(x, y1, legend=\"y=10^x\", fill_color=\"red\", line_color=\"red\", size=6)\n",
    "p.line(x, y2, legend=\"y=10^x^2\", line_color=\"orange\", line_dash=\"4 4\")\n",
    "\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Bar(y=[2, 3, 1]))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
